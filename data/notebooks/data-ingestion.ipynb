{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-14T06:25:16.430430500Z",
     "start_time": "2024-10-14T06:25:15.853389300Z"
    }
   },
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from dateutil import parser\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def parse_dates(date_string):\n",
    "    try:\n",
    "        return parser.parse(date_string)\n",
    "    except ValueError:\n",
    "        return pd.NaT  # Not a Time - pandas' equivalent of NaN for datetime"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-14T06:25:16.431430300Z",
     "start_time": "2024-10-14T06:25:16.429430400Z"
    }
   },
   "id": "3df093cf21aee04a",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "con = duckdb.connect(\"E:/duckdb/tennis.duckdb\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-14T06:25:16.444840200Z",
     "start_time": "2024-10-14T06:25:16.429430400Z"
    }
   },
   "id": "682c58d0976b136a",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Betfair - Market summaries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc9e6747353f790d"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ConnectionException",
     "evalue": "Connection Error: Connection already closed!",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mConnectionException\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[47], line 15\u001B[0m\n\u001B[0;32m     12\u001B[0m market_summ_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mevent_date\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m market_summ_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mevent_date\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(parse_dates)\n\u001B[0;32m     13\u001B[0m market_summ_df \u001B[38;5;241m=\u001B[39m market_summ_df\u001B[38;5;241m.\u001B[39mquery(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mevent_date <= \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2098-01-01\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mdrop(columns\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhandicap\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 15\u001B[0m \u001B[43mcon\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mDROP TABLE IF EXISTS market_summaries\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m con\u001B[38;5;241m.\u001B[39mexecute(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCREATE TABLE market_summaries AS SELECT * FROM market_summ_df\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mConnectionException\u001B[0m: Connection Error: Connection already closed!"
     ]
    }
   ],
   "source": [
    "market_summ = []\n",
    "for path in glob.glob(\"E:/Data/tennis/market-summaries/*\"):\n",
    "    market = pd.read_csv(path, encoding='latin-1', dtype={'market_id':'str'})\n",
    "    market_summ.append(market)\n",
    "    \n",
    "market_summ_df = pd.concat(market_summ)\n",
    "\n",
    "market_summ_df.loc[market_summ_df['market_id'].str.len() < 11, 'market_id'] = \\\n",
    "    market_summ_df.loc[market_summ_df['market_id'].str.len() < 11, 'market_id'].apply(lambda x: x.ljust(11, '0'))\n",
    "\n",
    "# Assuming your data is in a DataFrame called 'df'\n",
    "market_summ_df['event_date'] = market_summ_df['event_date'].apply(parse_dates)\n",
    "market_summ_df = market_summ_df.query('event_date <= \"2098-01-01\"').drop(columns='handicap')\n",
    "\n",
    "con.execute(\"DROP TABLE IF EXISTS market_summaries\")\n",
    "con.execute(\"CREATE TABLE market_summaries AS SELECT * FROM market_summ_df\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T06:29:48.689669800Z",
     "start_time": "2024-09-30T06:29:23.638476600Z"
    }
   },
   "id": "f345b9301aac4000",
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# con.execute(\"SELECT * FROM market_summaries LIMIT 100\").df()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T06:43:50.161849100Z",
     "start_time": "2024-09-30T06:43:50.149242200Z"
    }
   },
   "id": "3eb7ceb5d8c2e543",
   "execution_count": 64
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Betfair - Competition mappings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62f69640c311e0a8"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<duckdb.duckdb.DuckDBPyConnection at 0x1a864ec09f0>"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_mappings = []\n",
    "for path in glob.glob(\"E:/Data/tennis/competition-mapping/*\"):\n",
    "    mapping = pd.read_csv(path)\n",
    "    comp_mappings.append(mapping)\n",
    "    \n",
    "comp_mappings_df = pd.concat(comp_mappings)\n",
    "comp_mappings_df['market_id'] = \"1.\" + comp_mappings_df['MARKET_ID'].astype(str).str.pad(9, fillchar='0', side='right')\n",
    "\n",
    "comp_mappings_df.drop(columns='MARKET_ID',inplace=True)\n",
    "\n",
    "con.execute(\"DROP TABLE IF EXISTS competition_mappings\")\n",
    "con.execute(\"CREATE TABLE competition_mappings AS SELECT * FROM comp_mappings_df\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T06:33:44.104781600Z",
     "start_time": "2024-09-30T06:33:41.713554700Z"
    }
   },
   "id": "27c5ce84faaec3d1",
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# con.execute(\"SELECT * FROM competition_mappings LIMIT 100\").df()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T06:44:01.859486600Z",
     "start_time": "2024-09-30T06:44:01.851919300Z"
    }
   },
   "id": "93dde63fd9a6013e",
   "execution_count": 66
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sofascore - Events"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "640ed41464e3cec0"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3921/3921 [04:43<00:00, 13.82it/s]\n",
      "C:\\Users\\Vlad\\AppData\\Local\\Temp\\ipykernel_13744\\1776706592.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_df['datetime'] = combined_df['startTimestamp'].apply(datetime.fromtimestamp)\n"
     ]
    }
   ],
   "source": [
    "# Directory containing the JSON files\n",
    "directory = r'E:\\Data\\tennis\\sofascore\\events'\n",
    "\n",
    "# List to store DataFrames from each file\n",
    "dfs = []\n",
    "\n",
    "# Function to safely select columns\n",
    "def safe_select_columns(df, columns):\n",
    "    return df.reindex(columns=columns, fill_value=pd.NA)\n",
    "\n",
    "# Loop through all JSON files in the directory\n",
    "for filename in tqdm(os.listdir(directory)):\n",
    "    if filename.endswith('.json'):\n",
    "        # Extract event_fetch_date from filename\n",
    "        event_fetch_date = filename.split('_')[-1].split('.')[0]\n",
    "        \n",
    "        with open(os.path.join(directory, filename), 'r') as f:\n",
    "            data = json.load(f)\n",
    "        events = data['events']\n",
    "        df = pd.json_normalize(events)\n",
    "        # df = safe_select_columns(df, selected_columns)\n",
    "        \n",
    "        # Add event_fetch_date column\n",
    "        df['event_fetch_date'] = event_fetch_date\n",
    "        \n",
    "        dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "combined_df['datetime'] = combined_df['startTimestamp'].apply(datetime.fromtimestamp)\n",
    "\n",
    "filtered_df = (\n",
    "    combined_df.sort_values('event_fetch_date')\n",
    "    .groupby('id')\n",
    "    .last()\n",
    "    .reset_index()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T10:46:32.149649700Z",
     "start_time": "2024-09-30T10:40:45.166806600Z"
    }
   },
   "id": "1d4597d4e3af91ae",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e8ee5e3558e24472b2b1fedea073a94a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<duckdb.duckdb.DuckDBPyConnection at 0x298e218da30>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the desired columns\n",
    "selected_columns = ['id', 'startTimestamp', 'slug', 'groundType', 'tournament.uniqueTournament.name', 'tournament.category.name', 'tournament.uniqueTournament.tennisPoints', 'tournament.uniqueTournament.hasEventPlayerStatistics', 'season.name', 'season.year', 'roundInfo.name', 'status.description', 'homeTeam.name', 'homeTeam.slug', 'homeTeam.shortName', 'homeTeam.country.name', \n",
    "                    'awayTeam.name', 'awayTeam.slug', 'awayTeam.shortName', 'awayTeam.country.name', 'winnerCode',\n",
    "                    'homeScore.period1','homeScore.period2','homeScore.period3','homeScore.period4','homeScore.period5',\n",
    "                    'awayScore.period1','awayScore.period2','awayScore.period3','awayScore.period4','awayScore.period5', 'event_fetch_date', 'datetime']\n",
    "\n",
    "filtered_df = filtered_df[selected_columns].rename(columns={\n",
    "    'tournament.uniqueTournament.name': 'tournament_name', \n",
    "    'tournament.category.name': 'tournament_category', \n",
    "    'tournament.uniqueTournament.tennisPoints': 'tournament_points', \n",
    "    'tournament.uniqueTournament.hasEventPlayerStatistics': 'tournament_has_stats', \n",
    "    'season.name': 'season_name', \n",
    "    'season.year': 'season_year', \n",
    "    'roundInfo.name': 'tournament_round', \n",
    "    'status.description': 'match_status', \n",
    "    'homeTeam.name': 'home_team', \n",
    "    'homeTeam.slug': 'home_team_slug', \n",
    "    'homeTeam.shortName': 'home_team_short', \n",
    "    'hometeam.country.name': 'home_team_country', \n",
    "    'awayTeam.name': 'away_team', \n",
    "    'awayTeam.slug': 'away_team_slug', \n",
    "    'awayTeam.shortName': 'away_team_short', \n",
    "    'awayTeam.country.name': 'away_team_country',\n",
    "    'homeScore.period1': 'home_score_period1',\n",
    "    'homeScore.period2': 'home_score_period2',\n",
    "    'homeScore.period3': 'home_score_period3',\n",
    "    'homeScore.period4': 'home_score_period4',\n",
    "    'homeScore.period5': 'home_score_period5',\n",
    "    'awayScore.period1': 'away_score_period1',\n",
    "    'awayScore.period2': 'away_score_period2',\n",
    "    'awayScore.period3': 'away_score_period3',\n",
    "    'awayScore.period4': 'away_score_period4',\n",
    "    'awayScore.period5': 'away_score_period5'\n",
    "}).drop(columns='startTimestamp')\n",
    "\n",
    "con.execute(\"DROP TABLE IF EXISTS sofascore_events\")\n",
    "con.execute(\"CREATE TABLE sofascore_events AS SELECT * FROM filtered_df\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T10:53:52.147938900Z",
     "start_time": "2024-09-30T10:53:45.876023600Z"
    }
   },
   "id": "9ed0f19ff6874630",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "           id                                             slug groundType  \\\n0   4493462.0                       czech-republic-netherlands       None   \n1   4493464.0                                     japan-canada       None   \n2   4493466.0                                    germany-spain       None   \n3   4493468.0                                 australia-france       None   \n4   4493470.0                                great-britain-usa       None   \n..        ...                                              ...        ...   \n95  4843878.0                                  de-paula-turini       Clay   \n96  4843884.0  andreozzi-demoliner-ratiwatana-sa-ratiwatana-so       Clay   \n97  4843894.0                  alund-pella-kretschmer-satschko       Clay   \n98  4843896.0                     gonzalez-arguello-ghem-souza       Clay   \n99  4843898.0                cunha-romboli-cerretani-shamasdin       Clay   \n\n                  tournament_name tournament_category  tournament_points  \\\n0                       Davis Cup           Davis Cup                NaN   \n1                       Davis Cup           Davis Cup                NaN   \n2                       Davis Cup           Davis Cup                NaN   \n3                       Davis Cup           Davis Cup                NaN   \n4                       Davis Cup           Davis Cup                NaN   \n..                            ...                 ...                ...   \n95  Sao Paulo, Brazil Men Singles          Challenger                NaN   \n96   Sao Paulo, Brazil Men Double          Challenger                NaN   \n97   Sao Paulo, Brazil Men Double          Challenger                NaN   \n98   Sao Paulo, Brazil Men Double          Challenger                NaN   \n99   Sao Paulo, Brazil Men Double          Challenger                NaN   \n\n    tournament_has_stats     season_name season_year   tournament_round  ...  \\\n0                   True  Davis Cup 2010        2010        Round of 16  ...   \n1                  False  Davis Cup 2010        2010              Final  ...   \n2                  False  Davis Cup 2010        2010      Quarterfinals  ...   \n3                   True  Davis Cup 2010        2010         Semifinals  ...   \n4                   True  Davis Cup 2010        2010              Final  ...   \n..                   ...             ...         ...                ...  ...   \n95                 False            None        None  1/16-finals (R32)  ...   \n96                 False            None        None   1/8-finals (R16)  ...   \n97                  True            None        None   1/8-finals (R16)  ...   \n98                 False            None        None   1/8-finals (R16)  ...   \n99                 False            None        None   1/8-finals (R16)  ...   \n\n   home_score_period3 home_score_period4 home_score_period5  \\\n0                 NaN                NaN                NaN   \n1                 NaN                NaN                NaN   \n2                 NaN                NaN                NaN   \n3                 NaN                NaN                NaN   \n4                 NaN                NaN                NaN   \n..                ...                ...                ...   \n95                NaN                NaN                NaN   \n96               10.0                NaN                NaN   \n97                NaN                NaN                NaN   \n98                NaN                NaN                NaN   \n99               11.0                NaN                NaN   \n\n   away_score_period1 away_score_period2 away_score_period3  \\\n0                 NaN                NaN                NaN   \n1                 NaN                NaN                NaN   \n2                 NaN                NaN                NaN   \n3                 NaN                NaN                NaN   \n4                 NaN                NaN                NaN   \n..                ...                ...                ...   \n95                8.0                6.0                NaN   \n96                7.0                6.0                4.0   \n97                NaN                NaN                NaN   \n98                3.0                4.0                NaN   \n99                6.0                7.0                5.0   \n\n   away_score_period4 away_score_period5 event_fetch_date            datetime  \n0                 NaN                NaN       2014-02-01 2014-02-01 01:00:00  \n1                 NaN                NaN       2014-01-31 2014-01-31 17:30:00  \n2                 NaN                NaN       2014-02-01 2014-02-01 01:15:00  \n3                 NaN                NaN       2014-02-01 2014-01-31 23:45:00  \n4                 NaN                NaN       2014-02-01 2014-02-01 07:30:00  \n..                ...                ...              ...                 ...  \n95                NaN                NaN       2014-01-01 2013-12-31 23:55:00  \n96                NaN                NaN       2014-01-01 2013-12-31 23:50:00  \n97                NaN                NaN       2014-01-01 2014-01-01 00:15:00  \n98                NaN                NaN       2014-01-01 2014-01-01 01:00:00  \n99                NaN                NaN       2014-01-01 2014-01-01 03:15:00  \n\n[100 rows x 32 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>slug</th>\n      <th>groundType</th>\n      <th>tournament_name</th>\n      <th>tournament_category</th>\n      <th>tournament_points</th>\n      <th>tournament_has_stats</th>\n      <th>season_name</th>\n      <th>season_year</th>\n      <th>tournament_round</th>\n      <th>...</th>\n      <th>home_score_period3</th>\n      <th>home_score_period4</th>\n      <th>home_score_period5</th>\n      <th>away_score_period1</th>\n      <th>away_score_period2</th>\n      <th>away_score_period3</th>\n      <th>away_score_period4</th>\n      <th>away_score_period5</th>\n      <th>event_fetch_date</th>\n      <th>datetime</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4493462.0</td>\n      <td>czech-republic-netherlands</td>\n      <td>None</td>\n      <td>Davis Cup</td>\n      <td>Davis Cup</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>Davis Cup 2010</td>\n      <td>2010</td>\n      <td>Round of 16</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2014-02-01</td>\n      <td>2014-02-01 01:00:00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4493464.0</td>\n      <td>japan-canada</td>\n      <td>None</td>\n      <td>Davis Cup</td>\n      <td>Davis Cup</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>Davis Cup 2010</td>\n      <td>2010</td>\n      <td>Final</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2014-01-31</td>\n      <td>2014-01-31 17:30:00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4493466.0</td>\n      <td>germany-spain</td>\n      <td>None</td>\n      <td>Davis Cup</td>\n      <td>Davis Cup</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>Davis Cup 2010</td>\n      <td>2010</td>\n      <td>Quarterfinals</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2014-02-01</td>\n      <td>2014-02-01 01:15:00</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4493468.0</td>\n      <td>australia-france</td>\n      <td>None</td>\n      <td>Davis Cup</td>\n      <td>Davis Cup</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>Davis Cup 2010</td>\n      <td>2010</td>\n      <td>Semifinals</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2014-02-01</td>\n      <td>2014-01-31 23:45:00</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4493470.0</td>\n      <td>great-britain-usa</td>\n      <td>None</td>\n      <td>Davis Cup</td>\n      <td>Davis Cup</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>Davis Cup 2010</td>\n      <td>2010</td>\n      <td>Final</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2014-02-01</td>\n      <td>2014-02-01 07:30:00</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>4843878.0</td>\n      <td>de-paula-turini</td>\n      <td>Clay</td>\n      <td>Sao Paulo, Brazil Men Singles</td>\n      <td>Challenger</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>None</td>\n      <td>None</td>\n      <td>1/16-finals (R32)</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>8.0</td>\n      <td>6.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2014-01-01</td>\n      <td>2013-12-31 23:55:00</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>4843884.0</td>\n      <td>andreozzi-demoliner-ratiwatana-sa-ratiwatana-so</td>\n      <td>Clay</td>\n      <td>Sao Paulo, Brazil Men Double</td>\n      <td>Challenger</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>None</td>\n      <td>None</td>\n      <td>1/8-finals (R16)</td>\n      <td>...</td>\n      <td>10.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>7.0</td>\n      <td>6.0</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2014-01-01</td>\n      <td>2013-12-31 23:50:00</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>4843894.0</td>\n      <td>alund-pella-kretschmer-satschko</td>\n      <td>Clay</td>\n      <td>Sao Paulo, Brazil Men Double</td>\n      <td>Challenger</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>None</td>\n      <td>None</td>\n      <td>1/8-finals (R16)</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2014-01-01</td>\n      <td>2014-01-01 00:15:00</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>4843896.0</td>\n      <td>gonzalez-arguello-ghem-souza</td>\n      <td>Clay</td>\n      <td>Sao Paulo, Brazil Men Double</td>\n      <td>Challenger</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>None</td>\n      <td>None</td>\n      <td>1/8-finals (R16)</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2014-01-01</td>\n      <td>2014-01-01 01:00:00</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>4843898.0</td>\n      <td>cunha-romboli-cerretani-shamasdin</td>\n      <td>Clay</td>\n      <td>Sao Paulo, Brazil Men Double</td>\n      <td>Challenger</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>None</td>\n      <td>None</td>\n      <td>1/8-finals (R16)</td>\n      <td>...</td>\n      <td>11.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>6.0</td>\n      <td>7.0</td>\n      <td>5.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2014-01-01</td>\n      <td>2014-01-01 03:15:00</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 32 columns</p>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"SELECT * FROM sofascore_events LIMIT 100\").df()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T10:54:25.472530600Z",
     "start_time": "2024-09-30T10:54:25.439793800Z"
    }
   },
   "id": "e829cad1aad3a527",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sofascore - Match stats"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8043071ffc435b57"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5335/5335 [00:03<00:00, 1684.80it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def flatten_json(json_data, match_id):\n",
    "    rows = []\n",
    "    \n",
    "    for period in json_data.get('statistics', []):\n",
    "        period_name = period.get('period', '')\n",
    "        \n",
    "        for group in period.get('groups', []):\n",
    "            group_name = group.get('groupName', '')\n",
    "            \n",
    "            for item in group.get('statisticsItems', []):\n",
    "                row = {\n",
    "                    'match_id': match_id,\n",
    "                    'period': period_name,\n",
    "                    'group': group_name,\n",
    "                    'name': item.get('name', ''),\n",
    "                    'home': item.get('home', ''),\n",
    "                    'away': item.get('away', ''),\n",
    "                    'compareCode': item.get('compareCode', ''),\n",
    "                    'statisticsType': item.get('statisticsType', ''),\n",
    "                    'valueType': item.get('valueType', ''),\n",
    "                    'homeValue': item.get('homeValue', ''),\n",
    "                    'awayValue': item.get('awayValue', ''),\n",
    "                    'key': item.get('key', '')\n",
    "                }\n",
    "                \n",
    "                if 'homeTotal' in item:\n",
    "                    row['homeTotal'] = item['homeTotal']\n",
    "                if 'awayTotal' in item:\n",
    "                    row['awayTotal'] = item['awayTotal']\n",
    "                \n",
    "                rows.append(row)\n",
    "    \n",
    "    return rows\n",
    "\n",
    "def process_files(folder_path, con, chunk_size=10000):\n",
    "    all_data = []\n",
    "    table_created = True\n",
    "    \n",
    "    for file_path in tqdm(glob.glob(os.path.join(folder_path, '*.json'))):\n",
    "        match_id = os.path.splitext(os.path.basename(file_path))[0].split('_')[-1]\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                json_data = json.load(file)\n",
    "                all_data.extend(flatten_json(json_data, match_id))\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON in file {file_path}: {str(e)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "        \n",
    "        # If we've collected enough rows or it's the first chunk, process the data\n",
    "        if len(all_data) >= chunk_size or not table_created:\n",
    "            df = pd.DataFrame(all_data)\n",
    "            \n",
    "            if not table_created:\n",
    "                # Create the table using the first chunk of data\n",
    "                con.execute(\"DROP TABLE IF EXISTS sofascore_match_stats\")\n",
    "                con.execute(\"CREATE TABLE sofascore_match_stats AS SELECT * FROM df LIMIT 0\")\n",
    "                table_created = True\n",
    "            \n",
    "            # Insert the data\n",
    "            con.execute(\"INSERT INTO sofascore_match_stats SELECT * FROM df\")\n",
    "            all_data.clear()\n",
    "    \n",
    "    # Write any remaining data\n",
    "    if all_data:\n",
    "        df = pd.DataFrame(all_data)\n",
    "        con.execute(\"INSERT INTO sofascore_match_stats SELECT * FROM df\")\n",
    "\n",
    "# Usage\n",
    "folder_path = os.path.join('E:/', 'Data', 'tennis', 'sofascore', 'match-stats')\n",
    "\n",
    "# Process files and insert data in chunks\n",
    "process_files(folder_path, con)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-06T22:57:08.814426200Z",
     "start_time": "2024-10-06T22:57:05.601316900Z"
    }
   },
   "id": "9b80131cc1492f48",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sofascore - Point by point"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b773c6dd7b05397a"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5335/5335 [00:01<00:00, 2864.38it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<duckdb.duckdb.DuckDBPyConnection at 0x1a57cb73270>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flatten_json(json_data, match_id):\n",
    "    rows = []\n",
    "    \n",
    "    if 'pointByPoint' not in json_data or not json_data['pointByPoint']:\n",
    "        # print(f\"Warning: No point-by-point data found for match {match_id}\")\n",
    "        return rows\n",
    "\n",
    "    for set_data in json_data['pointByPoint']:\n",
    "        set_number = set_data.get('set', 'Unknown')\n",
    "        \n",
    "        for game in set_data.get('games', []):\n",
    "            game_number = game.get('game', 'Unknown')\n",
    "            \n",
    "            for point in game.get('points', []):\n",
    "                row = {\n",
    "                    'match_id': match_id,\n",
    "                    'set': set_number,\n",
    "                    'game': game_number,\n",
    "                    'homePoint': point.get('homePoint', 'Unknown'),\n",
    "                    'awayPoint': point.get('awayPoint', 'Unknown'),\n",
    "                    'pointDescription': point.get('pointDescription', 'Unknown'),\n",
    "                    'homePointType': point.get('homePointType', 'Unknown'),\n",
    "                    'awayPointType': point.get('awayPointType', 'Unknown')\n",
    "                }\n",
    "                rows.append(row)\n",
    "            \n",
    "            # Add a row for the game score if it exists\n",
    "            score = game.get('score')\n",
    "            if score:\n",
    "                row = {\n",
    "                    'match_id': match_id,\n",
    "                    'set': set_number,\n",
    "                    'game': game_number,\n",
    "                    'homePoint': 'GAME',\n",
    "                    'awayPoint': 'GAME',\n",
    "                    'pointDescription': -1,  # Use -1 to indicate this is a game score\n",
    "                    'homePointType': score.get('homeScore', 'Unknown'),\n",
    "                    'awayPointType': score.get('awayScore', 'Unknown'),\n",
    "                    'serving': score.get('serving', 'Unknown'),\n",
    "                    'scoring': score.get('scoring', 'Unknown')\n",
    "                }\n",
    "                rows.append(row)\n",
    "    \n",
    "    return rows\n",
    "\n",
    "def process_files(folder_path):\n",
    "    all_data = []\n",
    "    \n",
    "    for file_path in tqdm(glob.glob(os.path.join(folder_path, '*.json'))):\n",
    "        match_id = os.path.splitext(os.path.basename(file_path))[0].split('_')[-1]\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r') as file:\n",
    "                json_data = json.load(file)\n",
    "                all_data.extend(flatten_json(json_data, match_id))\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Warning: Unable to parse JSON in file {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "    \n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "# Usage\n",
    "folder_path = os.path.join('E:/', 'Data', 'tennis', 'sofascore', 'point-by-point')\n",
    "df = process_files(folder_path)\n",
    "\n",
    "# con.execute(\"DROP TABLE IF EXISTS sofascore_point_by_point\")\n",
    "con.execute(\"INSERT INTO sofascore_point_by_point SELECT * FROM df\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-06T23:02:34.880312600Z",
     "start_time": "2024-10-06T23:02:31.440472Z"
    }
   },
   "id": "90a65f4f2839172b",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cleaned sofascore point by point data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3253293f3aac0748"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def clean_pbp_data(pbp):\n",
    "    pbp.sort_values(['match_id', 'set', 'game'], inplace=True)\n",
    "\n",
    "    pbp['home_prev_point'] = pbp.groupby(['match_id', 'set', 'game'])['homePoint'].shift(1).fillna('0')\n",
    "    pbp['away_prev_point'] = pbp.groupby(['match_id', 'set', 'game'])['awayPoint'].shift(1).fillna('0')\n",
    "\n",
    "    pbp['homePoint_num'] = pd.to_numeric(pbp['homePoint'], errors='coerce')\n",
    "    pbp['awayPoint_num'] = pd.to_numeric(pbp['awayPoint'], errors='coerce')\n",
    "    pbp['home_prev_point_num'] = pd.to_numeric(pbp['home_prev_point'], errors='coerce')\n",
    "    pbp['away_prev_point_num'] = pd.to_numeric(pbp['away_prev_point'], errors='coerce')\n",
    "\n",
    "    pbp['homePointWinner'] = (\n",
    "            (pbp['homePoint_num'] > pbp['home_prev_point_num']) |\n",
    "            ((pbp['homePoint'] == \"A\") & (pbp['home_prev_point'] == \"40\")) |\n",
    "            ((pbp['away_prev_point'] == \"A\") & (pbp['awayPoint'] == \"40\")) |\n",
    "            ((pbp['homePoint'] == \"GAME\") & ((pbp['home_prev_point'] == \"A\") | (\n",
    "                    (pbp['home_prev_point'] == \"40\") & (pbp['away_prev_point'] != \"A\"))))\n",
    "    )\n",
    "\n",
    "    pbp['awayPointWinner'] = (\n",
    "            (pbp['awayPoint_num'] > pbp['away_prev_point_num']) |\n",
    "            ((pbp['awayPoint'] == \"A\") & (pbp['away_prev_point'] == \"40\")) |\n",
    "            ((pbp['home_prev_point'] == \"A\") & (pbp['homePoint'] == \"40\")) |\n",
    "            ((pbp['awayPoint'] == \"GAME\") & ((pbp['away_prev_point'] == \"A\") | (\n",
    "                    (pbp['away_prev_point'] == \"40\") & (pbp['home_prev_point'] != \"A\"))))\n",
    "    )\n",
    "\n",
    "    game_scores = pbp.query('pointDescription == -1')[\n",
    "        ['match_id', 'set', 'game', 'homePointType', 'awayPointType', 'serving']].drop_duplicates()\n",
    "\n",
    "    game_scores.sort_values(['match_id', 'set', 'game'], inplace=True)\n",
    "    game_scores['homeGames'] = game_scores.groupby(['match_id', 'set'])['homePointType'].shift(1).fillna(0)\n",
    "    game_scores['awayGames'] = game_scores.groupby(['match_id', 'set'])['awayPointType'].shift(1).fillna(0)\n",
    "    game_scores['homeSetWinner'] = (\n",
    "            (game_scores['homePointType'] == 6) & (game_scores['awayPointType'] <= 4) |\n",
    "            (game_scores['homePointType'] == 7) & (game_scores['awayPointType'].isin([5, 6])) |\n",
    "            (game_scores['homePointType'] > 7) & ((game_scores['homePointType'] - game_scores['awayPointType']) == 2)\n",
    "    )\n",
    "\n",
    "    game_scores['awaySetWinner'] = (\n",
    "            (game_scores['awayPointType'] == 6) & (game_scores['homePointType'] <= 4) |\n",
    "            (game_scores['awayPointType'] == 7) & (game_scores['homePointType'].isin([5, 6])) |\n",
    "            (game_scores['awayPointType'] > 7) & ((game_scores['awayPointType'] - game_scores['homePointType']) == 2)\n",
    "    )\n",
    "\n",
    "    game_scores['homeSets'] = game_scores.groupby('match_id')['homeSetWinner'].cumsum()\n",
    "    game_scores['homeSets'] = game_scores.groupby('match_id')['homeSets'].shift(1).fillna(0)\n",
    "    game_scores['awaySets'] = game_scores.groupby('match_id')['awaySetWinner'].cumsum()\n",
    "    game_scores['awaySets'] = game_scores.groupby('match_id')['awaySets'].shift(1).fillna(0)\n",
    "\n",
    "    game_scores['homeServing'] = game_scores['serving'] == 1.0\n",
    "    game_scores['awayServing'] = game_scores['serving'] == 2.0\n",
    "\n",
    "    pbp_merged = pbp.merge(game_scores[\n",
    "                               ['match_id', 'set', 'game', 'homeGames', 'awayGames', 'homeSets', 'awaySets',\n",
    "                                'homeServing',\n",
    "                                'awayServing']], on=['match_id', 'set', 'game'])\n",
    "\n",
    "    pbp_merged['homeMatchWinner'] = pbp_merged['winnerCode'] == 1.0\n",
    "    pbp_merged['awayMatchWinner'] = pbp_merged['winnerCode'] == 2.0\n",
    "\n",
    "    invalid_points_non_tb = ['1',\n",
    "                             '2',\n",
    "                             '3',\n",
    "                             '4',\n",
    "                             '5',\n",
    "                             '6',\n",
    "                             '7',\n",
    "                             '8',\n",
    "                             '9',\n",
    "                             '10',\n",
    "                             '11',\n",
    "                             '12',\n",
    "                             '13',\n",
    "                             '14',\n",
    "                             '16',\n",
    "                             '17',\n",
    "                             '18',\n",
    "                             '19',\n",
    "                             '20',\n",
    "                             '21',\n",
    "                             '22',\n",
    "                             '23',\n",
    "                             '41']\n",
    "\n",
    "    invalid_match_ids = pbp_merged[\n",
    "        ((pbp_merged['bo5'] == False) & ((pbp_merged['homeSets'] >= 2) | (pbp_merged['awaySets'] >= 2))) |\n",
    "        (pbp_merged['game'] != pbp_merged['homeGames'] + pbp_merged['awayGames'] + 1) |\n",
    "        (pbp_merged['set'] != pbp_merged['homeSets'] + pbp_merged['awaySets'] + 1) |\n",
    "        ((pbp_merged['game'] != 13) & (pbp_merged['home_prev_point'].isin(invalid_points_non_tb)))\n",
    "        ]['match_id'].unique().tolist()\n",
    "\n",
    "    pbp_merged['next_home_pt_is_game'] = pbp_merged.groupby('match_id')['homePoint'].shift(-1) == \"GAME\"\n",
    "    pbp_merged = pbp_merged.query('~(game == 13 and next_home_pt_is_game == True)').copy()\n",
    "\n",
    "    pbp_merged['home_win_tiebreak'] = ((pbp_merged['homePoint_num'] == 7.0) & (pbp_merged['awayPoint_num'] <= 5.0)) | (\n",
    "                (pbp_merged['homePoint_num'] > 7.0) & (\n",
    "                    pbp_merged['homePoint_num'] - pbp_merged['awayPoint_num']) >= 2.0)\n",
    "    pbp_merged['away_win_tiebreak'] = ((pbp_merged['awayPoint_num'] == 7.0) & (pbp_merged['homePoint_num'] <= 5.0)) | (\n",
    "                (pbp_merged['awayPoint_num'] > 7.0) & (\n",
    "                    pbp_merged['awayPoint_num'] - pbp_merged['homePoint_num']) >= 2.0)\n",
    "    pbp_merged['home_prev_win_tiebreak'] = ((pbp_merged['home_prev_point_num'] == 7.0) & (\n",
    "                pbp_merged['away_prev_point_num'] <= 5.0)) | ((pbp_merged['home_prev_point_num'] > 7.0) & (\n",
    "                pbp_merged['home_prev_point_num'] - pbp_merged['away_prev_point_num']) >= 2.0)\n",
    "    pbp_merged['away_prev_win_tiebreak'] = ((pbp_merged['away_prev_point_num'] == 7.0) & (\n",
    "                pbp_merged['home_prev_point_num'] <= 5.0)) | ((pbp_merged['away_prev_point_num'] > 7.0) & (\n",
    "                pbp_merged['away_prev_point_num'] - pbp_merged['home_prev_point_num']) >= 2.0)\n",
    "    pbp_merged = pbp_merged.query(\n",
    "        \"home_win_tiebreak == False and away_win_tiebreak == False and home_prev_win_tiebreak == False and away_prev_win_tiebreak == False\").copy()\n",
    "\n",
    "    pbp_merged = pbp_merged.query(\n",
    "        '~(homePoint == \"GAME\" and game == 13) and match_id not in @invalid_match_ids').copy().drop(columns=[\n",
    "        'away_prev_win_tiebreak', 'home_prev_win_tiebreak', 'away_win_tiebreak', 'home_win_tiebreak',\n",
    "        'next_home_pt_is_game', 'awayServing', 'home_prev_point_num', 'away_prev_point_num', 'homePoint_num',\n",
    "        'awayPoint_num',\n",
    "        'homePoint', 'awayPoint', 'pointDescription', 'homePointType', 'awayPointType', 'serving', 'scoring',\n",
    "        'winnerCode', 'tournament_category', 'tournament_points', 'bo5', 'awayMatchWinner', 'awayPointWinner',\n",
    "    ]).rename(columns={\n",
    "        'home_prev_point': 'home_game_score',\n",
    "        'away_prev_point': 'away_game_score',\n",
    "        'homePointWinner': 'home_point_winner',\n",
    "        'homeGames': 'home_games_won',\n",
    "        'awayGames': 'away_games_won',\n",
    "        'homeSets': 'home_sets_won',\n",
    "        'awaySets': 'away_sets_won',\n",
    "        'homeServing': 'home_serving',\n",
    "        'homeMatchWinner': 'home_match_winner'\n",
    "    })\n",
    "\n",
    "    pbp_merged['home_sets_won'] = pbp_merged['home_sets_won'].astype(int)\n",
    "    pbp_merged['away_sets_won'] = pbp_merged['away_sets_won'].astype(int)\n",
    "    pbp_merged['home_games_won'] = pbp_merged['home_games_won'].astype(int)\n",
    "    pbp_merged['away_games_won'] = pbp_merged['away_games_won'].astype(int)\n",
    "    pbp_merged['home_point_winner'] = pbp_merged['home_point_winner'].astype(int)\n",
    "    pbp_merged['home_serving'] = pbp_merged['home_serving'].astype(int)\n",
    "    pbp_merged['home_match_winner'] = pbp_merged['home_match_winner'].astype(int)\n",
    "\n",
    "    df = pbp_merged[['match_id', 'set', 'game', 'home_sets_won', 'away_sets_won', 'home_games_won', 'away_games_won',\n",
    "                       'home_game_score', 'away_game_score', 'home_serving', 'home_point_winner', 'home_match_winner']]\n",
    "\n",
    "    df_home = df.rename(columns={\n",
    "        'home_sets_won': 'sets_for',\n",
    "        'away_sets_won': 'sets_against',\n",
    "        'home_games_won': 'games_for',\n",
    "        'away_games_won': 'games_against',\n",
    "        'home_game_score': 'points_for',\n",
    "        'away_game_score': 'points_against',\n",
    "        'home_serving': 'serving',\n",
    "        'home_point_winner': 'point_winner',\n",
    "        'home_match_winner': 'match_winner'\n",
    "    })\n",
    "\n",
    "    df_home['position'] = 'home'\n",
    "\n",
    "    df_away = df.rename(columns={\n",
    "        'home_sets_won': 'sets_against',\n",
    "        'away_sets_won': 'sets_for',\n",
    "        'home_games_won': 'games_against',\n",
    "        'away_games_won': 'games_for',\n",
    "        'home_game_score': 'points_against',\n",
    "        'away_game_score': 'points_for',\n",
    "        'home_serving': 'serving',\n",
    "        'home_point_winner': 'point_winner',\n",
    "        'home_match_winner': 'match_winner'\n",
    "    })\n",
    "\n",
    "    df_away['position'] = 'away'\n",
    "    df_away['serving'] = (df_away['serving'] != 1).astype(int)\n",
    "    df_away['point_winner'] = (df_away['point_winner'] != 1).astype(int)\n",
    "    df_away['match_winner'] = (df_away['match_winner'] != 1).astype(int)\n",
    "\n",
    "    df_final = pd.concat([df_home, df_away])\n",
    "\n",
    "    return df_final\n",
    "\n",
    "\n",
    "def chunk_list(data, n):\n",
    "    # Split the list 'data' into chunks of size 'n'\n",
    "    for i in range(0, len(data), n):\n",
    "        yield data[i:i + n]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-14T06:26:39.306984Z",
     "start_time": "2024-10-14T06:26:39.294966600Z"
    }
   },
   "id": "2ff1716ebdd7ea7a",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<duckdb.duckdb.DuckDBPyConnection at 0x142affc88b0>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"DROP TABLE sofascore_point_by_point_clean\")\n",
    "con.execute(\"CREATE TABLE sofascore_point_by_point_clean AS SELECT * FROM pbp_clean\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-14T06:27:15.696308800Z",
     "start_time": "2024-10-14T06:27:15.484442500Z"
    }
   },
   "id": "a57c71ec2953613d",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<duckdb.duckdb.DuckDBPyConnection at 0x142affc88b0>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"TRUNCATE TABLE sofascore_point_by_point_clean\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-14T06:27:26.433403800Z",
     "start_time": "2024-10-14T06:27:26.419801800Z"
    }
   },
   "id": "9eda6b235b56bcf9",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 7/28 [01:00<04:58, 14.22s/it]"
     ]
    },
    {
     "data": {
      "text/plain": "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "64fc8fcbf0ec4784867c72ce01efd5dc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 8/28 [01:24<05:41, 17.09s/it]"
     ]
    },
    {
     "data": {
      "text/plain": "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "541fc79076154e839138539f8b8e8ac8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 24/28 [06:50<01:24, 21.07s/it]"
     ]
    },
    {
     "data": {
      "text/plain": "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a3a2cf73f6354a308406a409ec04794c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 25/28 [07:13<01:05, 21.69s/it]"
     ]
    },
    {
     "data": {
      "text/plain": "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "080ce5b418c54c8881f8f9a046ae3656"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 26/28 [07:38<00:45, 22.69s/it]"
     ]
    },
    {
     "data": {
      "text/plain": "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e492a6775daa4183ac648919b4093f5a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [08:06<00:00, 17.37s/it]\n"
     ]
    }
   ],
   "source": [
    "events = con.execute(\"SELECT id, winnerCode, tournament_category, tournament_points FROM sofascore_events\").df()\n",
    "\n",
    "event_ids = events['id'].astype(int).unique().tolist()\n",
    "chunked_event_ids = list(chunk_list(event_ids, 40000))\n",
    "\n",
    "for chk in tqdm(chunked_event_ids):\n",
    "    pbp = con.execute(f\"\"\"\n",
    "    SELECT p.*,  e.winnerCode, e.tournament_category, e.tournament_points\n",
    "    FROM sofascore_point_by_point p \n",
    "    INNER JOIN sofascore_events e ON p.match_id = e.id\n",
    "    WHERE p.match_id IN ({','.join(map(str, chk))})\n",
    "    \"\"\").df()\n",
    "\n",
    "    pbp['bo5'] = (pbp['tournament_category'] == \"ATP\") & (pbp['tournament_points'] == 2000.0)\n",
    "\n",
    "    pbp_clean = clean_pbp_data(pbp)\n",
    "    con.execute(\"INSERT INTO sofascore_point_by_point_clean SELECT * FROM pbp_clean\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-14T06:35:40.388748900Z",
     "start_time": "2024-10-14T06:27:33.793661500Z"
    }
   },
   "id": "8af021920d346a30",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "con.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-14T06:36:31.990408400Z",
     "start_time": "2024-10-14T06:36:31.703353600Z"
    }
   },
   "id": "e1168d8681e6731b",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f13e06e02d33166c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
